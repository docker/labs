:imagesdir: images

[[Docker_Swarm]]
== Deploy Application on Docker Swarm Cluster

Docker Swarm is native clustering for Docker. It allows you to create and access a pool of Docker hosts using the full suite of Docker tools. Because Docker Swarm serves the standard Docker API, any tool that already communicates with a Docker daemon can use Swarm to transparently scale to multiple hosts.

=== Key Components of Docker Swarm

.Key Components of Docker Swarm
image::docker-swarm-components.png[]

*Swarm Manager*: Docker Swarm has a Manager that is a pre-defined Docker Host and is a single point for all administration. Swarm manager orchestrates and schedules containers in the entire cluster. Swarm Manager can be configured to enable https://docs.docker.com/swarm/multi-manager-setup/[High Availability].

*Swarm Nodes*: The containers are deployed on Nodes that are additional Docker Hosts. Each Swarm Node must be accessible by the manager, each node must listen to the same network interface (TCP port). Each node runs a Docker Swarm agent that registers the referenced Docker daemon, monitors it, and updates the discovery backend with the node’s status. The containers run on a node.

*Scheduler Strategy*: Different scheduler strategies ("`binpack`", "`spread`" (default), and "`random`") can be applied to pick the best node to run your container. The default strategy optimizes the node for least number of running containers. There are multiple kinds of filters, such as constraints and affinity.  This should allow for a decent scheduling algorithm.

*Node Discovery Service*: Swarm manager talks to a hosted discovery service. This service maintains a list of IPs in the Swarm cluster. Docker Hub hosts a discovery service that can be used during development. In production, this is replaced by other services such as `etcd`, `consul`, or `zookeeper`. You can even use a static file. This is particularly useful if there is no Internet access or you are running in a closed network.

*Standard Docker API*: Docker Swarm serves the standard Docker API and thus any tool that talks to a single Docker host will seamlessly scale to multiple hosts now. That means that if you were using shell scripts using Docker CLI to configure multiple Docker hosts, the same CLI could now talk to the Swarm cluster.

There are a lots of other concepts but these are the main ones.

This section will deploy an application that will provide a CRUD/REST interface on a data bucket in http://developer.couchbase.com/server[Couchbase]. This is achieved by using a Java EE application deployed on http://wildfly.org[WildFly] to access the database.

=== Create Discovery Service

Create a Machine that will host discovery service:

[[Create_Consul_Docker_Machine]]
.Create Consul Docker Machine
====
[source, text]
----
docker-machine create -d=virtualbox consul-machine
----
====

Connect to this Machine:

[[Connect_to_Consul_Docker_Machine]]
.Connect to Consul Docker Machine
[source, text]
----
eval $(docker-machine env consul-machine)
----

Run Consul service using the following Compose file:

[[Consul_Docker_Compose_Definition]]
.Consul Docker Compose Definition
====
[source, text]
----
myconsul:
  image: progrium/consul
  restart: always
  hostname: consul
  ports:
    - 8500:8500
  command: "-server -bootstrap"
----
====

Create a new directory, create a new file in that directory and name it `docker-compose.yml`, copy the contents from <<Consul_Docker_Compose_Definition>>. Start this service as:

[[Start_Docker_Compose_Service]]
.Start Consul Service
====
[source, text]
----
docker-compose up -d
----
====

Started container can be verified as:

[[Verify_Consul_Service]]
.Verify Consul Service
====
[source, text]
----
docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                                                            NAMES
5aba992e29c4        progrium/consul     "/bin/start -server -"   20 seconds ago      Up 19 seconds       53/tcp, 53/udp, 8300-8302/tcp, 8400/tcp, 8301-8302/udp, 0.0.0.0:8500->8500/tcp   tmp_myconsul_1
----
====

=== Create Docker Swarm Cluster

Swarm is fully integrated with Machine, and so is the easiest way to get started.

==== Create Swarm Master Node

Create a Swarm Master using the Consul discovery service:

[[Create_Docker_Machine_for_Swarm_Master]]
.Create Docker Machine for Swarm Master
====
[source, text]
----
docker-machine create -d virtualbox --swarm --swarm-master --swarm-discovery="consul://$(docker-machine ip consul-machine):8500" --engine-opt="cluster-store=consul://$(docker-machine ip consul-machine):8500" --engine-opt="cluster-advertise=eth1:2376" swarm-master
====

Few options to look here:

. `--swarm` configures the Machine with Swarm
. `--swarm-master` configures the created Machine to be Swarm master
. `--swarm-discovery` defines address of the discovery service
. `--cluster-advertise` advertise the machine on the network
. `--cluster-store` designate a distributed k/v storage backend for the cluster

Find some information about this machine as `docker-machine inspect --format='{{json .Driver}}'  swarm-master | jq`:

[[Swarm_Master_Docker_Machine_Info]]
.Swarm Master Docker Machine Info
[source, json]
----
{
  "Boot2DockerImportVM": "",
  "Boot2DockerURL": "",
  "CPU": 1,
  "DNSProxy": false,
  "DiskSize": 5000,
  "HostDNSResolver": false,
  "HostOnlyCIDR": "192.168.99.1/24",
  "HostOnlyNicType": "82540EM",
  "HostOnlyPromiscMode": "deny",
  "IPAddress": "192.168.99.104",
  "MachineName": "swarm-master",
  "Memory": 1024,
  "NoShare": false,
  "NoVTXCheck": false,
  "SSHKeyPath": "/Users/arungupta/.docker/machine/machines/swarm-master/id_rsa",
  "SSHPort": 57723,
  "SSHUser": "docker",
  "StorePath": "/Users/arungupta/.docker/machine",
  "SwarmDiscovery": "consul://192.168.99.100:8500",
  "SwarmHost": "tcp://0.0.0.0:3376",
  "SwarmMaster": true,
  "VBoxManager": {}
}
----

The disk size is defined by `DiskSize` attribute and its value is 5GB.

NOTE: `jq` can be installed as `brew install jq`.

==== Create Swarm Worker Nodes

Create the first Swarm node to join this cluster:

[[Docker_Swarm_Cluster_Node_1]]
.Docker Swarm Cluster Node 1
====
[source, text]
----
docker-machine create -d virtualbox --swarm --swarm-discovery="consul://$(docker-machine ip consul-machine):8500" --engine-opt="cluster-store=consul://$(docker-machine ip consul-machine):8500" --engine-opt="cluster-advertise=eth1:2376" swarm-node-01
----
====

Notice no `--swarm-master` is specified in this command. This ensure that the created Machines are _worker_ nodes.

Create a second Swarm node to join this cluster:

[[Docker_Swarm_Cluster_Node_2]]
.Docker Swarm Cluster Node 2
====
[source, text]
----
docker-machine create -d virtualbox --swarm --swarm-discovery="consul://$(docker-machine ip consul-machine):8500" --engine-opt="cluster-store=consul://$(docker-machine ip consul-machine):8500" --engine-opt="cluster-advertise=eth1:2376" swarm-node-02
----
====

This concludes the creation of Docker Swarm cluster. If you want to recreate the entire cluster again:

- Stop the existing machines:

    docker-machine stop swarm-node-02 swarm-node-01 swarm-master consul-machine`

- Remove the existing machines

    docker-machine rm swarm-node-02 swarm-node-01 swarm-master consul-machine`

- Create the cluster using https://gist.github.com/arun-gupta/c42cacfa3225727f5c71ff4a5dc547dc


==== Get Information about Swarm Cluster

Connect to the Swarm cluster by using the command:

[[Connect_to_Docker_Swarm_Cluster]]
.Connect to Docker Swarm Cluster
====
[source, text]
----
eval "$(docker-machine env --swarm swarm-master)"
----
====

`--swarm` is specified to connect to the Swarm cluster. Otherwise the command will connect to `swarm-master` Machine only. 

If you're on Windows, then use the `docker-machine env --swarm swarm-master` command only. Then copy the output into an editor to replace all appearances of EXPORT with SET, remove the quotes, and all appearances of "/" with "\". Finally, issue the three commands at your command prompt.

List all the created Machines using `docker-machine ls` command:

[[Docker_Machines_in_Swarm_Cluster]]
.Docker Machines in Swarm Cluster
====
[source, text]
----
NAME             ACTIVE      DRIVER       STATE     URL                         SWARM                   DOCKER    ERRORS
consul-machine   -           virtualbox   Running   tcp://192.168.99.100:2376                           v1.10.2   
swarm-master     * (swarm)   virtualbox   Running   tcp://192.168.99.104:2376   swarm-master (master)   v1.10.2   
swarm-node-01    -           virtualbox   Running   tcp://192.168.99.105:2376   swarm-master            v1.10.2   
swarm-node-02    -           virtualbox   Running   tcp://192.168.99.106:2376   swarm-master            v1.10.2   
----
====

The machines that are part of the cluster have cluster’s name in the SWARM column. If the SWARM column is blank, then it is a standalone machine. For example, `consul-machine` is a standalone machine as opposed to all other machines which are part of the `swarm-master` cluster. The Swarm master is identified by (master) in the SWARM column.


Get information about the cluster using the `docker info` command:

[[Docker_Swarm_Cluster_Information]]
.Docker Swarm Cluster Information
====
[source, text]
----
Containers: 4
 Running: 4
 Paused: 0
 Stopped: 0
Images: 3
Server Version: swarm/1.1.3
Role: primary
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 3
 swarm-master: 192.168.99.104:2376
  └ Status: Healthy
  └ Containers: 2
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.021 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.1.18-boot2docker, operatingsystem=Boot2Docker 1.10.2 (TCL 6.4.1); master : 611be10 - Mon Feb 22 22:47:06 UTC 2016, provider=virtualbox, storagedriver=aufs
  └ Error: (none)
  └ UpdatedAt: 2016-03-09T02:05:15Z
 swarm-node-01: 192.168.99.105:2376
  └ Status: Healthy
  └ Containers: 1
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.021 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.1.18-boot2docker, operatingsystem=Boot2Docker 1.10.2 (TCL 6.4.1); master : 611be10 - Mon Feb 22 22:47:06 UTC 2016, provider=virtualbox, storagedriver=aufs
  └ Error: (none)
  └ UpdatedAt: 2016-03-09T02:05:44Z
 swarm-node-02: 192.168.99.106:2376
  └ Status: Healthy
  └ Containers: 1
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.021 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.1.18-boot2docker, operatingsystem=Boot2Docker 1.10.2 (TCL 6.4.1); master : 611be10 - Mon Feb 22 22:47:06 UTC 2016, provider=virtualbox, storagedriver=aufs
  └ Error: (none)
  └ UpdatedAt: 2016-03-09T02:05:39Z
Plugins: 
 Volume: 
 Network: 
Kernel Version: 4.1.18-boot2docker
Operating System: linux
Architecture: amd64
CPUs: 3
Total Memory: 3.064 GiB
Name: swarm-master
----
====

This cluster has 3 nodes – one Swarm master and 2 Swarm worker nodes. There are a total of 4 containers running in this cluster – a swarm-agent on each node and an additional swarm-agent-master running on the master. This can be verified by connecting to the master Machine (without specifying `--swarm`) and listing all the containers:

[[Containers_on_Docker_Swarm_Master]]
.Containers on Docker Swarm Master
====
[source, text]
----
> eval "$(docker-machine env swarm-master)"
> docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
64c7be815898        swarm:latest        "/swarm join --advert"   16 minutes ago      Up 13 minutes                           swarm-agent
ac9808732975        swarm:latest        "/swarm manage --tlsv"   16 minutes ago      Up 13 minutes                           swarm-agent-master
----
====

You can also query the Consul discovery service to find the list of nodes in the cluster using:

[source, text]
----
docker run swarm list consul://$(docker-machine ip consul-machine):8500
----

This will show the output as:

[[Docker_Swarm_Node_List]]
.Docker Swarm Node List
=====
[source, text]
----
time="2016-03-08T23:50:03Z" level=info msg="Initializing discovery without TLS" 
192.168.99.101:2376
192.168.99.102:2376
192.168.99.103:2376
----
=====

=== Start Application using Docker Compose

Connect to the Swarm cluster as explained in <<Connect_to_Docker_Swarm_Cluster>>.

Let's verify the list of networks created first by using `docker network ls` command:

[[Docker_Network_List]]
.Docker Network List
====
[source, text]
----
NETWORK ID          NAME                   DRIVER
429c2425a228        swarm-node-02/none     null                
b37990ade9a1        swarm-node-02/host     host                
c6ee7250f273        swarm-node-01/host     host                
78cd88ea2799        swarm-node-02/bridge   bridge              
26ebe5e2ae7d        swarm-master/none      null                
a559fe0ae472        swarm-master/host      host                
199983a7c616        swarm-node-01/bridge   bridge              
03139227e25e        swarm-node-01/none     null                
43dba5c86a3a        swarm-master/bridge    bridge 
----
====

Docker creates three networks for each host:

[options="header", cols="1,3", width="80%"]
|====
| Network Name | Purpose
| `bridge` | Default network that containers connect to.
| `none` | Container-specific networking stack
| `host` | Adds a container on hosts networking stack. Network configuration is identical to the host.
|====

A total of nine networks are created for this three-node Swarm cluster.

Create a new directory and `cd` to it:

    mkdir compose-swarm
    cd compose-swarm

Create a new Compose definition using the Compose file shown below. This will start WildFly and Couchbase:

[[WildFly_and_Couchbase_Compose_Definition]]
.WildFly and Couchbase Compose Definition
====
[source, text]
----
version: '2'
services:
  mycouchbase:
    container_name: "db"
    image: couchbase
    ports:
      - 8091:8091
      - 8092:8092 
      - 8093:8093 
      - 11210:11210
  mywildfly:
    image: arungupta/wildfly-admin
    container_name: "web"
    environment:
      - COUCHBASE_URI=db
    ports:
      - 8080:8080
      - 9990:9990
----
====

In this Compose file:

. `couchbase` image is used for Couchbase server.
. `arungupta/wildfly-admin` image is used as it binds WildFly’s management to all network interfaces, and in addition also exposes port 9990. This enables WildFly Maven Plugin to be used to deploy the application.
. Both services have a custom container name defined by `container_name` attribute. Database container name is specified as a new environment variable `COUCHBASE_URI` during WildFly startup.

This application can be started as:

[source, text]
----
docker-compose up -d
----

WildFly and Couchbase containers are started on two separate worker nodes (based upon the default `spread` distribution strategy).

A new `docker_gwbridge` network is also created on each node that have application containers running. This network allows the containers to have external connectivity outside of their cluster, and is created on each worker node.

A new overlay network is created. This allows multiple containers on different hosts to communicate with each other.

Read more about https://docs.docker.com/engine/userguide/networking/dockernetworks/[Docker Networks].

=== Verify Containers in Application

Connect to the Swarm cluster and verify that WildFly and Couchbase are running using `docker-compose ps`:

[[Swarm_Containers_using_Docker_Compose]]
.Swarm Containers using Docker Compose
====
[source, text]
----
docker-compose ps
             Name                             Command                            State                             Ports              
-------------------------------------------------------------------------------------------------------------------------------------
             Name                             Command                            State                             Ports              
-------------------------------------------------------------------------------------------------------------------------------------
db                                /entrypoint.sh /opt/couchb ...    Up                                11207/tcp, 192.168.99.106:11210 
                                                                                                      ->11210/tcp, 11211/tcp,         
                                                                                                      18091/tcp, 18092/tcp,           
                                                                                                      192.168.99.106:8091->8091/tcp,  
                                                                                                      192.168.99.106:8092->8092/tcp,  
                                                                                                      192.168.99.106:8093->8093/tcp   
web                               /opt/jboss/wildfly/bin/sta ...    Up                                192.168.99.105:8080->8080/tcp,  
                                                                                                      192.168.99.105:9990->9990/tcp  
----
====

Exact host for each container can be seen using `docker ps` command:

[[Swarm_Containers_using_docker_ps]]
.Swarm Containers using `docker ps`
====
[source, text]
----
CONTAINER ID        IMAGE                     COMMAND                  CREATED             STATUS              PORTS                                                                                                             NAMES
3c7a3e8bb69c        couchbase                 "/entrypoint.sh couch"   13 minutes ago      Up 13 minutes       192.168.99.106:8091-8093->8091-8093/tcp, 11207/tcp, 11211/tcp, 192.168.99.106:11210->11210/tcp, 18091-18092/tcp   swarm-node-02/db
e2ccef549b1d        arungupta/wildfly-admin   "/opt/jboss/wildfly/b"   14 minutes ago      Up 14 minutes       192.168.99.105:8080->8080/tcp, 192.168.99.105:9990->9990/tcp                                                      swarm-node-01/web
----
====

The Couchbase server is running on `swarm-node-01` node and WildFly is running on `swarm-node-02`. Take a note on which nodes your Couchbase and WildFly servers are running and update the following commands accordingly.

=== Configure Couchbase server

Clone https://github.com/arun-gupta/couchbase-javaee.git. This workspace contains a simple Java EE application that is deployed on WildFly and provides a REST API over a sample bucket in Couchbase.

Couchbase server can be configured using http://developer.couchbase.com/documentation/server/current/rest-api/rest-endpoints-all.html[Couchbase REST API]. The application contains a Maven profile that allows to configure the Couchbase server and loads the `travel-sample` bucket. This can be invoked as: (Note that you may need to replace `swarm-node-02` with the node in your cluster that is running Couchbase)

[[Configure_Couchbase_Server]]
.Configure Couchbase Server
====
[source, text]
----
mvn install -Pcouchbase -Ddocker.host=$(docker-machine ip swarm-node-02)

. . .

* Server auth using Basic with user 'Administrator'
> POST /sampleBuckets/install HTTP/1.1
> Authorization: Basic QWRtaW5pc3RyYXRvcjpwYXNzd29yZA==

. . .

} [data not shown]
* upload completely sent off: 17 out of 17 bytes
< HTTP/1.1 202 Accepted
* Server Couchbase Server is not blacklisted
< Server: Couchbase Server

. . .
----
====

=== Deploy Application

Deploy the application to WildFly by specifying three parameters:

. Host IP address where WildFly is running (`swarm-node-01` in this example but update as needed for your cluster)
. Username of a user in WildFly's administrative realm
. Password of the user specified in WildFly's administrative realm

[[Deploy_Application_to_WildFly]]
.Deploy Application to WildFly
====
[source, text]
----
mvn install -Pwildfly -Dwildfly.hostname=$(docker-machine ip swarm-node-01) -Dwildfly.username=admin -Dwildfly.password=Admin#007

. . .

Nov 29, 2015 12:11:14 AM org.xnio.Xnio <clinit>
INFO: XNIO version 3.3.1.Final
Nov 29, 2015 12:11:14 AM org.xnio.nio.NioXnio <clinit>
INFO: XNIO NIO Implementation Version 3.3.1.Final
Nov 29, 2015 12:11:15 AM org.jboss.remoting3.EndpointImpl <clinit>
INFO: JBoss Remoting version 4.0.9.Final
[INFO] Authenticating against security realm: ManagementRealm
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------

. . .
----
====

=== Access Application

Now that the WildFly and Couchbase servers have been configured, let's access the application. You need to specify the IP address of the Machine where WildFly is running (`swarm-node-01` in this example but update as needed for your cluster).

The endpoint can be accessed in this case as:

    curl http://$(docker-machine ip swarm-node-01):8080/airlines/resources/airline

The output is shown as:

[[Java_EE_Application_Output]]
.Java EE Application Output
====
[source, text]
----
[{"travel-sample":{"id":10123,"iata":"TQ","icao":"TXW","name":"Texas Wings","callsign":"TXW","type":"airline","country":"United States"}}, {"travel-sample":{"id":10642,"iata":null,"icao":"JRB","name":"Jc royal.britannica","callsign":null,"type":"airline","country":"United Kingdom"}}, {"travel-sample":{"id":112,"iata":"5W","icao":"AEU","name":"Astraeus","callsign":"FLYSTAR","type":"airline","country":"United Kingdom"}}, {"travel-sample":{"id":1355,"iata":"BA","icao":"BAW","name":"British Airways","callsign":"SPEEDBIRD","type":"airline","country":"United Kingdom"}}, {"travel-sample":{"id":10765,"iata":"K5","icao":"SQH","name":"SeaPort Airlines","callsign":"SASQUATCH","type":"airline","country":"United States"}}, {"travel-sample":{"id":13633,"iata":"WQ","icao":"PQW","name":"PanAm World Airways","callsign":null,"type":"airline","country":"United States"}}, {"travel-sample":{"id":139,"iata":"SB","icao":"ACI","name":"Air Caledonie International","callsign":"AIRCALIN","type":"airline","country":"France"}}, {"travel-sample":{"id":13391,"iata":"-+","icao":"--+","name":"U.S. Air","callsign":null,"type":"airline","country":"United States"}}, {"travel-sample":{"id":1191,"iata":"UU","icao":"REU","name":"Air Austral","callsign":"REUNION","type":"airline","country":"France"}}, {"travel-sample":{"id":1316,"iata":"FL","icao":"TRS","name":"AirTran Airways","callsign":"CITRUS","type":"airline","country":"United States"}}]
----
====


